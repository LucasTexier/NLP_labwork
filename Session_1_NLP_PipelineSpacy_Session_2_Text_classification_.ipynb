{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbLip68I6RlK"
      },
      "source": [
        "The objective during the following 5 3-hour practical session this week will be to explore different tools\n",
        "for important NLP tasks.\n",
        " \n",
        "The first two sessions will focus on the NLP Pipeline and how to apply it for Text Classification. \n",
        "We will first learn how to create NLP Pipelines with SpaCy and define the successive execution of NLP tools to achieve:\n",
        "1. Loading Models\n",
        "2. Loading and Normalization\n",
        "    1. Loading text and documents\n",
        "    2. Segmenting into tokens and sentences\n",
        "    3. Stop-word filtering\n",
        "    4. Lemmatization and Stemming\n",
        "    5. Computing counts and frequencies\n",
        "3. Tasks\n",
        "    1. Part of Speech Tagging\n",
        "    2. Parsing\n",
        "        1. Dependency\n",
        "        2. Shallow\n",
        "    3. Named Entity Recognition \n",
        "4. Rule-based Matching\n",
        "5. Custom pipelines\n",
        "\n",
        "\n",
        "After we've explored the capabilities of pipelines with SpaCy in the first tutorial session, In the second session, we will complete a tutorial (read/watch https://huggingface.co/tasks/text-classification, get the notebook: https://github.com/huggingface/notebooks/blob/main/examples/text_classification.ipynb) on text classification with hugging-face transformers and then adapt both approaches for text classification on a new dataset: \n",
        "\n",
        "- A traditional ML pipeline using scikit-learn and feature engineering based on NLP features (e.g. using spacy). You can combine TFIdf features with your own one-hot encoded linguistic features (scikit-learn can combine different types of features using pipelines)\n",
        "\n",
        "- A transformer architecture as studied in the hunggingface tutorial. \n",
        "\n",
        "You will have to adjust the approaches to train/evaluate them on the Yelp Reviews dataset: https://www.yelp.com/dataset\n",
        "\n",
        "Please establish a protocol to compare and evaluate the two systems using relevant metrics. You may consider several transformer models and see if task-specific models are better (they should be).  \n",
        "\n",
        "\n",
        "For the feature engineering aspect, you could proceed as follows: \n",
        "   1. Explore the corpus and create an NLP pipeline to prepare the text for processing\n",
        "   2. Identify features that are likely to be informative for the classification task\n",
        "   3. Create feature vectors from the linguistic features using sci-kit learn\n",
        "   4. Set up a series of classifiers and compare them to determine optimal parameters and feature combinations    \n",
        "\n",
        "\n",
        "This notebook is adapted from several online resources:\n",
        "- Source 1 https://realpython.com/natural-language-processing-spacy-python/\n",
        "- Source 2 https://www.ekino.com/articles/simple-nlp-tasks-tutorial\n",
        "- Source 3 https://spacy.io/usage/processing-pipelines\n",
        "# I. NLP Pipelines\n",
        "## 1. Loading models\n",
        "We first need to download a set of models that we can use with Spacy. There is a limited number of \n",
        "supported languages, but Englih and French are among them. You may consult the Spacy documentation to see a list of available models:\n",
        "[https://spacy.io/usage/models](https://spacy.io/usage/models)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "gSlI7OKu6RlN",
        "pycharm": {
          "is_executing": false,
          "name": "#%% md\n"
        }
      },
      "source": [
        "Install Spacy English Model\n",
        "\n",
        "```\n",
        "python -m spacy download en_core_web_md\n",
        "```\n",
        "\n",
        "There's also a french model that can be loaded in the same way if you need it:\n",
        "\n",
        "```\n",
        "python -m spacy download fr_core_news_md\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9geFW1Ic6ykb",
        "outputId": "4a9008ed-3e4d-4ebb-84c9-7b410af61883"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-01-11 09:17:21.850199: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-lg==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.4.1/en_core_web_lg-3.4.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-lg==3.4.1) (3.4.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.25.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (8.1.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.11)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.10.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (21.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.4.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.1)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.4.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHCG8Vrk6RlO",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "We first need to download a set of models that we can use with SpaCy. There is a limited number of \n",
        "supported languages, but Englih and French are among them. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "ivbpMiqU6RlP",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_lg') # Load english models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nppd-nKI6RlQ",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## 2. Loading and Normalization\n",
        "### 2.1 Loading texts and doucuments\n",
        "Now that we know how to load models, we can see how to perform the first basic steps of the NLP Pipeline: reading and processing a text.  \n",
        "\n",
        "We can either process a string directly or load the text from a file. \n",
        "Then we must process the text with Spacy, which gives us a `Doc` instance from which we can retrieve successive annotations. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MAjdYBgN6RlQ",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# From a string \n",
        "introduction_text = ('This tutorial is about Natural Language Processing in Spacy.')\n",
        "introduction_doc = nlp(introduction_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1n6sdxKU6RlR",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "#From a file\n",
        "file_name = 'introduction.txt'\n",
        "introduction_file_text = open(file_name).read()\n",
        "introduction_file_doc = nlp(introduction_file_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKk7TPmE6RlR",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### 2.2 Segmenting into tokens and sentences\n",
        "#### Sentence detection\n",
        "**Sentence Detection** is the process of locating the start and end of sentences \n",
        "in a given text. This allows you to you divide a text into linguistically meaningful \n",
        "units. You’ll use these units when you’re processing your text to perform tasks such\n",
        "as **part of speech tagging** and **entity extraction**.\n",
        "\n",
        "In spaCy, the sents property is used to extract sentences. \n",
        "Here’s how you would extract the total number of sentences and the sentences \n",
        "for a given input text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IjgDacW6RlR",
        "outputId": "f06c71bd-2091-49a4-c50f-14a1da508d86",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SENT: Gus Proto is a Python developer currently working for a London-based Fintech company.\n",
            "SENT: He is interested in learning Natural Language Processing.\n"
          ]
        }
      ],
      "source": [
        "about_text = ('Gus Proto is a Python developer currently'\n",
        "               ' working for a London-based Fintech'\n",
        "              ' company. He is interested in learning'\n",
        "              ' Natural Language Processing.')\n",
        "\n",
        "about_doc = nlp(about_text)\n",
        "sentences = list(about_doc.sents)\n",
        "len(sentences)\n",
        "\n",
        "for sentence in sentences:\n",
        "    print (\"SENT: \"+str(sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chKFvlDb6RlS",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "#### Tokenization\n",
        "\n",
        "**Tokenization** is the next step after sentence detection. \n",
        "It allows you to identify the basic units in your text. \n",
        "These basic units are called **tokens**. \n",
        "Tokenization is useful because it breaks a text into meaningful units.\n",
        " These units are used for further analysis, like part of speech tagging.\n",
        "\n",
        "In spaCy, you can print tokens by iterating on the Doc object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCC2AQBi6RlS",
        "outputId": "bf5e9332-f40f-449e-a75c-229cea2aa6f6",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gus 0\n",
            "Proto 4\n",
            "is 10\n",
            "a 13\n",
            "Python 15\n",
            "developer 22\n",
            "currently 32\n",
            "working 42\n",
            "for 50\n",
            "a 54\n",
            "London 56\n",
            "- 62\n",
            "based 63\n",
            "Fintech 69\n",
            "company 77\n",
            ". 84\n",
            "He 86\n",
            "is 89\n",
            "interested 92\n",
            "in 103\n",
            "learning 106\n",
            "Natural 115\n",
            "Language 123\n",
            "Processing 132\n",
            ". 142\n"
          ]
        }
      ],
      "source": [
        "for token in about_doc:\n",
        "    print (token, token.idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uipryyKU6RlT",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Note how spaCy preserves the starting index of the tokens. \n",
        "It’s useful for in-place word replacement. \n",
        "spaCy provides [various](https://spacy.io/api/token#attributes) attributes for the Token class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydK60i9W6RlT",
        "outputId": "814bc6f1-7f79-401f-e9b1-ae601d27be5a",
        "pycharm": {
          "name": "#%% \n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gus 0 Gus  True False False Xxx False\n",
            "Proto 4 Proto  True False False Xxxxx False\n",
            "is 10 is  True False False xx True\n",
            "a 13 a  True False False x True\n",
            "Python 15 Python  True False False Xxxxx False\n",
            "developer 22 developer  True False False xxxx False\n",
            "currently 32 currently  True False False xxxx False\n",
            "working 42 working  True False False xxxx False\n",
            "for 50 for  True False False xxx True\n",
            "a 54 a  True False False x True\n",
            "London 56 London True False False Xxxxx False\n",
            "- 62 - False True False - False\n",
            "based 63 based  True False False xxxx False\n",
            "Fintech 69 Fintech  True False False Xxxxx False\n",
            "company 77 company True False False xxxx False\n",
            ". 84 .  False True False . False\n",
            "He 86 He  True False False Xx True\n",
            "is 89 is  True False False xx True\n",
            "interested 92 interested  True False False xxxx False\n",
            "in 103 in  True False False xx True\n",
            "learning 106 learning  True False False xxxx False\n",
            "Natural 115 Natural  True False False Xxxxx False\n",
            "Language 123 Language  True False False Xxxxx False\n",
            "Processing 132 Processing True False False Xxxxx False\n",
            ". 142 . False True False . False\n"
          ]
        }
      ],
      "source": [
        "for token in about_doc:\n",
        "    print (token, token.idx, token.text_with_ws,\n",
        "           token.is_alpha, token.is_punct, \n",
        "           token.is_space,token.shape_, token.is_stop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfse38eI6RlT",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "In this example, some of the commonly required attributes are accessed:\n",
        "\n",
        " - `text_with_ws` prints token text with trailing space (if present).\n",
        " - `is_alpha` detects if the token consists of alphabetic characters or not.\n",
        " - `is_punct` detects if the token is a punctuation symbol or not.\n",
        " - `is_space` detects if the token is a space or not.\n",
        " - `shape_` prints out the shape of the word.\n",
        " - `is_stop` detects if the token is a stop word or not.\n",
        "\n",
        "**Note:** *We will see stop word filtering in the next section* \n",
        "\n",
        "You can also customize the tokenization process to detect tokens on custom characters. \n",
        "This is often used for hyphenated words, which are words joined with hyphen. \n",
        "For example, “London-based” is a hyphenated word.\n",
        "\n",
        "spaCy allows you to customize tokenization by updating the tokenizer property on the `nlp` object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzUYkR736RlT",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "custom_nlp = spacy.load('en_core_web_sm')\n",
        "prefix_re = spacy.util.compile_prefix_regex(custom_nlp.Defaults.prefixes)\n",
        "suffix_re = spacy.util.compile_suffix_regex(custom_nlp.Defaults.suffixes)\n",
        "infix_re = re.compile(r'''[-~]''')\n",
        "def customize_tokenizer(nlp):\n",
        "    #Adds support to use `-` as the delimiter for tokenization\n",
        "    return Tokenizer(nlp.vocab, \n",
        "                     prefix_search=prefix_re.search,suffix_search=suffix_re.search,\n",
        "                     infix_finditer=infix_re.finditer,token_match=None)\n",
        "\n",
        "custom_nlp.tokenizer = customize_tokenizer(custom_nlp)\n",
        "custom_tokenizer_about_doc = custom_nlp(about_text)\n",
        "print([token.text for token in custom_tokenizer_about_doc])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2P01UQ56RlU",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "In order for you to customize, you can pass various parameters to the Tokenizer class:\n",
        " - `nlp.vocab` is a storage container for special cases and is used to handle cases like contractions and emoticons.\n",
        " - `prefix_search` is the function that is used to handle preceding punctuation, such as opening parentheses.\n",
        " - `infix_finditer` is the function that is used to handle non-whitespace separators, such as hyphens.\n",
        " - `suffix_search` is the function that is used to handle succeeding punctuation, such as closing parentheses.\n",
        " - `token_match` is an optional boolean function that is used to match strings that should never be split. \n",
        " \n",
        "It overrides the previous rules and is useful for entities like URLs or numbers.\n",
        "\n",
        "**Note:** *spaCy already detects hyphenated words as individual tokens. The above code is just an example to show how tokenization can be customized. It can be used for any other character.*\n",
        "\n",
        "### 2.3 Stop-word filtering\n",
        "Stop words are the most common words in a language. \n",
        "In the English language, some examples of stop words are `the`, `are`, `but`, and `they`. \n",
        "Most sentences need to contain stop words in order to be full sentences that make sense.\n",
        "\n",
        "Generally, stop words are removed because they aren’t significant and distort the word \n",
        "frequency analysis. spaCy has a list of stop words for the English language:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqThYvGd6RlU",
        "outputId": "3ff9840f-78e2-411a-d0e1-81526a6c1058",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "326"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import spacy\n",
        "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "len(spacy_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFILWIsv6RlU",
        "outputId": "1f380506-f835-4918-84ad-36c6fc20a5f6",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "give\n",
            "two\n",
            "while\n",
            "nowhere\n",
            "‘s\n",
            "become\n",
            "forty\n",
            "'s\n",
            "they\n",
            "beyond\n"
          ]
        }
      ],
      "source": [
        "for stop_word in list(spacy_stopwords)[:10]:\n",
        "     print(stop_word)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b67-w2E6RlU"
      },
      "source": [
        "You can remove stop words from the input text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylVvPVDf6RlV",
        "outputId": "228580f0-41e4-4802-ebeb-b60792eb498c",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gus\n",
            "Proto\n",
            "Python\n",
            "developer\n",
            "currently\n",
            "working\n",
            "London\n",
            "-\n",
            "based\n",
            "Fintech\n",
            "company\n",
            ".\n",
            "interested\n",
            "learning\n",
            "Natural\n",
            "Language\n",
            "Processing\n",
            ".\n"
          ]
        }
      ],
      "source": [
        "for token in about_doc:\n",
        "    if not token.is_stop:\n",
        "        print(token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCZQ5pii6RlV",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Stop words like `is`, `a`, `for`, `the`, and `in` are not printed in the output above. You can also create a list of tokens not containing stop words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yt-FmlOq6RlV",
        "outputId": "c3962ed6-2e2e-47e9-a0c7-85282b5c9f03",
        "pycharm": {
          "name": "#%% \n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Gus, Proto, Python, developer, currently, working, London, -, based, Fintech, company, ., interested, learning, Natural, Language, Processing, .]\n"
          ]
        }
      ],
      "source": [
        "about_no_stopword_doc = [token for token in about_doc if not token.is_stop]\n",
        "print (about_no_stopword_doc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sTWQAQ46RlV",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### 2.4 Lemmatization and Stemming\n",
        "#### Lemmatization\n",
        "**Lemmatization** is the process of reducing inflected forms of a word while\n",
        " still ensuring that the reduced form belongs to the language. \n",
        " This reduced form or root word is called a **lemma**.\n",
        "\n",
        "For example, *organizes*, *organized* and *organizing* are all forms of *organize*. \n",
        "Here, *organize* is the lemma. The inflection of a word allows you to express different \n",
        "grammatical categories like tense (organized vs organize), \n",
        "number (trains vs train), and so on. \n",
        "Lemmatization is necessary because it helps you reduce the inflected forms \n",
        "of a word so that they can be analyzed as a single item. \n",
        "It can also help you **normalize** the text.\n",
        "\n",
        "spaCy has the attribute `lemma_` on the Token class. This attribute has the lemmatized form of a token:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71R9j3Pi6RlW",
        "outputId": "8efdbf6c-ae44-4fae-f665-57a7d8d20fc4",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gus Gus\n",
            "is be\n",
            "helping helping\n",
            "organize organize\n",
            "a a\n",
            "developerconference developerconference\n",
            "on on\n",
            "Applications Applications\n",
            "of of\n",
            "Natural Natural\n",
            "Language Language\n",
            "Processing Processing\n",
            ". .\n",
            "He he\n",
            "keeps keep\n",
            "organizing organize\n",
            "local local\n",
            "Python Python\n",
            "meetups meetup\n",
            "and and\n",
            "several several\n",
            "internal internal\n",
            "talks talk\n",
            "at at\n",
            "his his\n",
            "workplace workplace\n",
            ". .\n"
          ]
        }
      ],
      "source": [
        "conference_help_text = ('Gus is helping organize a developer'\n",
        "    'conference on Applications of Natural Language'\n",
        "    ' Processing. He keeps organizing local Python meetups'\n",
        "    ' and several internal talks at his workplace.')\n",
        "conference_help_doc = nlp(conference_help_text)\n",
        "for token in conference_help_doc:\n",
        "    print (token, token.lemma_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qD_vPea6RlW"
      },
      "source": [
        "In this example, *organizing* reduces to its lemma form *organize*. \n",
        "If you do not lemmatize the text, \n",
        "then *organize* and *organizing* will be counted as different tokens, \n",
        "even though they both have a similar meaning. \n",
        "Lemmatization helps you avoid duplicate words that have similar meanings.\n",
        "\n",
        "#### Stemming\n",
        "spaCy cannot do stemming, but we can use the older reference NLP library, NLTK to import the Porter\n",
        "stemmer for Python. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwRrmUP_6RlW",
        "outputId": "1453a27f-e3f2-4c29-a776-cf6ebed0c702",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "program  :  program\n",
            "programs  :  program\n",
            "programer  :  program\n",
            "programing  :  program\n",
            "programers  :  program\n"
          ]
        }
      ],
      "source": [
        "# import these modules \n",
        "from nltk.stem import PorterStemmer \n",
        "\n",
        "ps = PorterStemmer() \n",
        "#equivalent to ps = SnowballStemmer(\"english\")\n",
        "# also available for french \n",
        "  \n",
        "# choose some words to be stemmed \n",
        "words = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"] \n",
        "  \n",
        "for w in words: \n",
        "    print(w, \" : \", ps.stem(w)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvjDy8-k6RlW",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Note that NLTK requires to download ressources for some of its components other than Stemmers: \n",
        "[https://www.nltk.org/data.html](https://www.nltk.org/data.html)\n",
        "\n",
        "## 2.5 Computing counts and frequencies\n",
        "You can now convert a given text into tokens and perform statistical analysis over it. \n",
        "This analysis can give you various insights about word patterns, such as common words or unique words in the text:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D--G8tmX6RlW",
        "outputId": "d3c2d027-9ff7-4c4e-f16a-9382ed542bec",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Gus', 4), ('London', 3), ('Natural', 3), ('Language', 3), ('Processing', 3)]\n",
            "['Proto', 'currentlyworking', 'based', 'company', 'interested', 'conference', 'happening', '21', 'July', '2019', 'titled', 'Applications', 'helpline', 'number', 'available', '+1', '1234567891', 'helping', 'organize', 'keeps', 'organizing', 'local', 'meetups', 'internal', 'talks', 'workplace', 'presenting', 'introduce', 'reader', 'Use', 'cases', 'Apart', 'work', 'passionate', 'music', 'play', 'enrolled', 'weekend', 'batch', 'situated', 'Mayfair', 'City', 'world', 'class', 'piano', 'instructors']\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "complete_text = ('Gus Proto is a Python developer currently'\n",
        "    'working for a London-based Fintech company. He is'\n",
        "    ' interested in learning Natural Language Processing.'\n",
        "    ' There is a developer conference happening on 21 July'\n",
        "    ' 2019 in London. It is titled \"Applications of Natural'\n",
        "    ' Language Processing\". There is a helpline number '\n",
        "    ' available at +1-1234567891. Gus is helping organize it.'\n",
        "    ' He keeps organizing local Python meetups and several'\n",
        "    ' internal talks at his workplace. Gus is also presenting'\n",
        "    ' a talk. The talk will introduce the reader about \"Use'\n",
        "    ' cases of Natural Language Processing in Fintech\".'\n",
        "    ' Apart from his work, he is very passionate about music.'\n",
        "    ' Gus is learning to play the Piano. He has enrolled '\n",
        "    ' himself in the weekend batch of Great Piano Academy.'\n",
        "    ' Great Piano Academy is situated in Mayfair or the City'\n",
        "    ' of London and has world-class piano instructors.')\n",
        "\n",
        "complete_doc = nlp(complete_text)\n",
        "# Remove stop words and punctuation symbols\n",
        "words = [token.text for token in complete_doc\n",
        "         if not token.is_stop and not token.is_punct]\n",
        "word_freq = Counter(words)\n",
        "# 5 commonly occurring words with their frequencies\n",
        "common_words = word_freq.most_common(5)\n",
        "print (common_words)\n",
        "\n",
        "# Unique words\n",
        "unique_words = [word for (word, freq) in word_freq.items() if freq == 1]\n",
        "print (unique_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFjDeQ146RlW"
      },
      "source": [
        "By looking at the common words, you can see that the text as a whole is probably about Gus, \n",
        "London, or Natural Language Processing. \n",
        "This way, you can take any unstructured text and perform statistical analysis to \n",
        "know what it’s about.\n",
        "\n",
        "Here’s another example of the same text with stop words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nm2Qs1GY6RlX",
        "outputId": "e99c53ed-71b4-460a-ab0e-12e8d519108a",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('is', 10), ('a', 5), ('in', 5), ('Gus', 4), ('of', 4)]\n"
          ]
        }
      ],
      "source": [
        "words_all = [token.text for token in complete_doc if not token.is_punct]\n",
        "word_freq_all = Counter(words_all)\n",
        "# 5 commonly occurring words with their frequencies\n",
        "common_words_all = word_freq_all.most_common(5)\n",
        "print (common_words_all)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-3mhbW26RlX",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Four out of five of the most common words are stop words, which don’t tell you much about the text. If you consider stop words while doing word frequency analysis, then you won’t be able to derive meaningful insights from the input text. This is why removing stop words is so important.\n",
        "## 3. Visualization\n",
        "spaCy comes with a built-in visualizer called displaCy. \n",
        "You can use it to visualize a dependency parse or named entities in a browser or \n",
        "a Jupyter notebook.\n",
        "\n",
        "You can use displaCy to find POS tags for tokens.\n",
        "In a browser:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OgaW_MNK6RlX",
        "pycharm": {
          "name": "#%% \n"
        }
      },
      "outputs": [],
      "source": [
        "from spacy import displacy\n",
        "about_interest_text = ('He is interested in learning'\n",
        "    ' Natural Language Processing.')\n",
        "about_interest_doc = nlp(about_interest_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtEJjU156RlX",
        "outputId": "bda68f2f-75c6-47ce-ea56-b23bce37249d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Using the 'dep' visualizer\n",
            "Serving on http://0.0.0.0:5000 ...\n",
            "\n",
            "Shutting down server on port 5000.\n"
          ]
        }
      ],
      "source": [
        "displacy.serve(about_interest_doc, style='dep')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC11Wo8a6RlX",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "The above code will spin a simple web server. \n",
        "You can see the visualization by opening [http://127.0.0.1:5000](http://127.0.0.1:5000) in your browser:\n",
        "\n",
        "![](https://files.realpython.com/media/displacy_pos_tags.45059f2bf851.png)\n",
        "\n",
        "You can also directly render the result in Jupyter: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "kfsHmXhs6RlX",
        "outputId": "0dda55d2-79fc-48ab-fdad-6c9301a3c94d",
        "pycharm": {
          "name": "#%% \n"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"b38febc57a7a4949a9957f63c82f9549-0\" class=\"displacy\" width=\"1450\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">He</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">interested</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">in</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">learning</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">Natural</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Language</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">Processing.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-b38febc57a7a4949a9957f63c82f9549-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-b38febc57a7a4949a9957f63c82f9549-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-b38febc57a7a4949a9957f63c82f9549-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-b38febc57a7a4949a9957f63c82f9549-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M395.0,179.0 L403.0,167.0 387.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-b38febc57a7a4949a9957f63c82f9549-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-b38febc57a7a4949a9957f63c82f9549-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-b38febc57a7a4949a9957f63c82f9549-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-b38febc57a7a4949a9957f63c82f9549-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-b38febc57a7a4949a9957f63c82f9549-0-4\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-b38febc57a7a4949a9957f63c82f9549-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M945,179.0 L937,167.0 953,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-b38febc57a7a4949a9957f63c82f9549-0-5\" stroke-width=\"2px\" d=\"M1120,177.0 C1120,89.5 1270.0,89.5 1270.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-b38febc57a7a4949a9957f63c82f9549-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1120,179.0 L1112,167.0 1128,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-b38febc57a7a4949a9957f63c82f9549-0-6\" stroke-width=\"2px\" d=\"M770,177.0 C770,2.0 1275.0,2.0 1275.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-b38febc57a7a4949a9957f63c82f9549-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1275.0,179.0 L1283.0,167.0 1267.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "displacy.render(about_interest_doc, style='dep', jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEda55QB6RlY",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## 4. Tasks\n",
        "In this part we will present an overview of the NLP tasks included in spaCy that are part of the \n",
        "standard NLP pipleine.\n",
        "\n",
        "### 4.1. Part of Speech Tagging\n",
        "\n",
        "Part of speech or POS is a grammatical role that explains how a particular word is used in a sentence. \n",
        "There are many different sets of tags for POS tagging, the most common for English is the Brow Corpus tagset that's \n",
        "being used in the Penn TreeBank. There are normally 21 tags, but they can be regrouped in the following broad categories: \n",
        "\n",
        "- Noun\n",
        "- Pronoun\n",
        "- Adjective\n",
        "- Verb\n",
        "- Adverb\n",
        "- Preposition\n",
        "- Conjunction\n",
        "- Interjection\n",
        "\n",
        "**Part of speech tagging** is the process of assigning a **POS tag** to each token depending \n",
        "on its usage in the sentence. \n",
        "POS tags are useful for assigning a syntactic category like noun or verb to each word.\n",
        "\n",
        "In spaCy, POS tags are available as an attribute on the Token object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et2DL9756RlY",
        "outputId": "24f475ec-2dd1-445b-c83c-38e7c9efbfc8",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gus NNP PROPN noun, proper singular\n",
            "Proto NNP PROPN noun, proper singular\n",
            "is VBZ AUX verb, 3rd person singular present\n",
            "a DT DET determiner\n",
            "Python NNP PROPN noun, proper singular\n",
            "developer NN NOUN noun, singular or mass\n",
            "currently RB ADV adverb\n",
            "working VBG VERB verb, gerund or present participle\n",
            "for IN ADP conjunction, subordinating or preposition\n",
            "a DT DET determiner\n",
            "London NNP PROPN noun, proper singular\n",
            "- HYPH PUNCT punctuation mark, hyphen\n",
            "based VBN VERB verb, past participle\n",
            "Fintech NNP PROPN noun, proper singular\n",
            "company NN NOUN noun, singular or mass\n",
            ". . PUNCT punctuation mark, sentence closer\n",
            "He PRP PRON pronoun, personal\n",
            "is VBZ AUX verb, 3rd person singular present\n",
            "interested JJ ADJ adjective (English), other noun-modifier (Chinese)\n",
            "in IN ADP conjunction, subordinating or preposition\n",
            "learning VBG VERB verb, gerund or present participle\n",
            "Natural NNP PROPN noun, proper singular\n",
            "Language NNP PROPN noun, proper singular\n",
            "Processing NN NOUN noun, singular or mass\n",
            ". . PUNCT punctuation mark, sentence closer\n"
          ]
        }
      ],
      "source": [
        "for token in about_doc:\n",
        "    print (token, token.tag_, token.pos_, spacy.explain(token.tag_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKSHeDaM6RlY"
      },
      "source": [
        "Here, two attributes of the Token class are accessed:\n",
        "\n",
        "1. `tag_ lists` the fine-grained part of speech.\n",
        "2. `pos_` lists the coarse-grained part of speech.\n",
        "3. `spacy.explain` gives descriptive details about a particular POS tag. spaCy provides a complete tag list along with an explanation for each tag.\n",
        "\n",
        "Using POS tags, you can extract a particular category of words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqxfKPsM6RlY",
        "outputId": "491dcf1e-f7fe-4ab9-e58b-0e474373b2c2",
        "pycharm": {
          "name": "#%% \n"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[interested]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nouns = []\n",
        "adjectives = []\n",
        "for token in about_doc:\n",
        "    if token.pos_ == 'NOUN':\n",
        "        nouns.append(token)\n",
        "    if token.pos_ == 'ADJ':\n",
        "        adjectives.append(token)\n",
        "\n",
        "nouns\n",
        "adjectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEiw2rUK6RlY",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "You can use this to derive insights, \n",
        "remove the most common nouns, \n",
        "or see which adjectives are used for a particular noun.\n",
        "### 4.2. Parsing\n",
        "spaCy only supports Dependency parsing, which we will examine first.  We cannot use spaCy to get a full constituency parse,\n",
        "howeve the use of third party libraries can allow to have a shallow consituency parsing. \n",
        "\n",
        "#### 4.2.1 Dependency\n",
        "**Dependency parsing** is the process of extracting the dependency parse of a sentence to represent \n",
        "its grammatical structure. It defines the dependency relationship between **headwords** and their \n",
        "**dependents**. The head of a sentence has no dependency and is called the **root of the sentence**. \n",
        "The **verb** is usually the head of the sentence. All other words are linked to the headword.\n",
        "\n",
        "The dependencies can be mapped in a directed graph representation:\n",
        "\n",
        "- Words are the nodes.\n",
        "- The grammatical relationships are the edges.\n",
        "Dependency parsing helps you know what role a word plays in the text and how different words \n",
        "relate to each other. It’s also used in **shallow parsing** and **named entity recognition**.\n",
        "\n",
        "Here’s how you can use dependency parsing to see the relationships between words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3FhhKb56RlY",
        "outputId": "55206f14-9603-4f95-9200-48ab1b8c41a8",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gus NNP learning nsubj\n",
            "is VBZ learning aux\n",
            "learning VBG learning ROOT\n",
            "piano NN learning dobj\n"
          ]
        }
      ],
      "source": [
        "piano_text = 'Gus is learning piano'\n",
        "piano_doc = nlp(piano_text)\n",
        "for token in piano_doc:\n",
        "    print (token.text, token.tag_, token.head.text, token.dep_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEzvCH0d6RlZ"
      },
      "source": [
        "In this example, the sentence contains three relationships:\n",
        "\n",
        "- `nsubj` is the subject of the word. Its headword is a verb.\n",
        "- `aux` is an auxiliary word. Its headword is a verb.\n",
        "- `dobj` is the direct object of the verb. Its headword is a verb.\n",
        "\n",
        "There is a detailed list of relationships with descriptions. \n",
        "\n",
        "You can visualise the result as follows: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "BNxMaHSm6RlZ",
        "outputId": "8f637eff-72b0-4faa-b1f3-b02cd8d169a2",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"ec31415f14a34ce1811d126e87b68f64-0\" class=\"displacy\" width=\"750\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Gus</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">learning</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">piano</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-ec31415f14a34ce1811d126e87b68f64-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,2.0 400.0,2.0 400.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-ec31415f14a34ce1811d126e87b68f64-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-ec31415f14a34ce1811d126e87b68f64-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-ec31415f14a34ce1811d126e87b68f64-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-ec31415f14a34ce1811d126e87b68f64-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-ec31415f14a34ce1811d126e87b68f64-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "displacy.render(piano_doc, style='dep', jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuDPwD8m6RlZ",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "This image shows you that the subject of the sentence is the proper noun `Gus` and \n",
        "that it has a learn relationship with `piano`.\n",
        "##### Navigating the Tree and Subtree\n",
        "The dependency parse tree has all the properties of a tree. \n",
        "This tree contains information about sentence structure and grammar and can be traversed\n",
        " in different ways to extract relationships.\n",
        "\n",
        "spaCy provides attributes like children, lefts, rights, and subtree to navigate the parse tree:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49GLWDvJ6RlZ",
        "outputId": "793cd38a-51c2-424f-e1f8-f1873b9dc710",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['a', 'Python', 'working']\n",
            "Python\n",
            "currently\n",
            "['a', 'Python']\n",
            "['working']\n",
            "[a, Python, developer, currently, working, for, a, London, -, based, Fintech, company]\n"
          ]
        }
      ],
      "source": [
        "one_line_about_text = ('Gus Proto is a Python developer'\n",
        "    ' currently working for a London-based Fintech company')\n",
        "one_line_about_doc = nlp(one_line_about_text)\n",
        "# Extract children of `developer`\n",
        "print([token.text for token in one_line_about_doc[5].children])\n",
        "\n",
        "# Extract previous neighboring node of `developer`\n",
        "print (one_line_about_doc[5].nbor(-1))\n",
        "\n",
        "# Extract next neighboring node of `developer`\n",
        "print (one_line_about_doc[5].nbor())\n",
        "\n",
        "# Extract all tokens on the left of `developer`\n",
        "print([token.text for token in one_line_about_doc[5].lefts])\n",
        "\n",
        "# Extract tokens on the right of `developer`\n",
        "print([token.text for token in one_line_about_doc[5].rights])\n",
        "\n",
        "# Print subtree of `developer`\n",
        "print (list(one_line_about_doc[5].subtree))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt0sjASf6Rla"
      },
      "source": [
        "You can construct a function that takes a subtree as an argument and returns a string by merging words in it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHdGHMsP6Rla",
        "outputId": "945687ba-8233-4357-ec2f-fb66cf63c6c7",
        "pycharm": {
          "name": "#%% \n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a Python developer currently working for a London-based Fintech company\n"
          ]
        }
      ],
      "source": [
        "def flatten_tree(tree):\n",
        "    return ''.join([token.text_with_ws for token in list(tree)]).strip()\n",
        "\n",
        "# Print flattened subtree of `developer`\n",
        "print (flatten_tree(one_line_about_doc[5].subtree))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVY3K6Ar6Rla",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "You can use this function to print all the tokens in a subtree.\n",
        "#### 4.2.2 Shallow\n",
        "**Shallow parsing**, or **chunking**, is the process of extracting phrases from unstructured text. \n",
        "Chunking groups adjacent tokens into phrases on the basis of their POS tags.\n",
        "There are some standard well-known chunks such as noun phrases, verb phrases, and prepositional phrases.\n",
        "##### Noun Phrase Detection\n",
        "\n",
        "A noun phrase is a phrase that has a noun as its head. \n",
        "It could also include other kinds of words, such as adjectives, ordinals, determiners. \n",
        "Noun phrases are useful for explaining the context of the sentence. \n",
        "They help you infer *what* is being talked about in the sentence.\n",
        "\n",
        "spaCy has the property `noun_chunks` on Doc object. You can use it to extract noun phrases:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J-xpAd76Rlb",
        "outputId": "4f6f09f4-fc78-4b27-eabb-075a2d056180",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a developer conference\n",
            "21 July\n",
            "London\n"
          ]
        }
      ],
      "source": [
        "conference_text = 'There is a developer conference happening on 21 July 2019 in London.'\n",
        "conference_doc = nlp(conference_text)\n",
        "# Extract Noun Phrases\n",
        "for chunk in conference_doc.noun_chunks:\n",
        "    print (chunk)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHVSZ7VF6Rlb",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "By looking at noun phrases, you can get information about your text.\n",
        "For example, a developer conference indicates that the text mentions a conference, \n",
        "while the date 21 July lets you know that conference is scheduled for 21 July. \n",
        "You can figure out whether the conference is in the past or the future.\n",
        "London tells you that the conference is in London.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhCl8vaj6Rlb",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "\n",
        "### 4.3 Named Entity Recognition\n",
        "\n",
        "**Named Entity Recognition (NER)** is the process of locating **named entities** in unstructured text and then classifying them into pre-defined categories, such as person names, organizations, locations, monetary values, percentages, time expressions, and so on.\n",
        "\n",
        "You can use **NER** to know more about the meaning of your text. \n",
        "For example, you could use it to populate tags for a set of documents in order to \n",
        "improve the keyword search. You could also use it to categorize customer support tickets\n",
        " into relevant categories.\n",
        "\n",
        "spaCy has the property `ents` on `Doc` objects. You can use it to extract named entities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU6yAO0w6Rlb",
        "outputId": "26166ef3-7d51-4ba4-9f54-437fd8e6fe0f",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Great Piano Academy 0 19 ORG Companies, agencies, institutions, etc.\n",
            "Mayfair 35 42 GPE Countries, cities, states\n",
            "the City of London 46 64 GPE Countries, cities, states\n"
          ]
        }
      ],
      "source": [
        "piano_class_text = ('Great Piano Academy is situated'\n",
        "    ' in Mayfair or the City of London and has'\n",
        "    ' world-class piano instructors.')\n",
        "piano_class_doc = nlp(piano_class_text)\n",
        "for ent in piano_class_doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char,\n",
        "          ent.label_, spacy.explain(ent.label_))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17d2aqih6Rlb",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "In the above example, `ent` is a Span object with various attributes:\n",
        "\n",
        "- `text` gives the Unicode text representation of the entity.\n",
        "- `start_char` denotes the character offset for the start of the entity.\n",
        "- `end_char` denotes the character offset for the end of the entity.\n",
        "- `label_` gives the label of the entity.\n",
        "\n",
        "`spacy.explain` gives descriptive details about an entity label. \n",
        "The spaCy model has a pre-trained list of entity classes. \n",
        "You can use displaCy to visualize these entities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "sYI8saVa6Rlb",
        "outputId": "64c210e2-1023-4df0-81fd-0822615ca067",
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Great Piano Academy\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " is situated in \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Mayfair\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " or \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the City of London\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " and has world-class piano instructors.</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "displacy.render(piano_class_doc, style=\"ent\", jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wwV3EEk6Rlc",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "You can use NER to redact people’s names from a text. \n",
        "For example, you might want to do this in order to hide personal information collected in a survey.\n",
        "You can use spaCy to do that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4gXVzl1g6Rlc",
        "outputId": "b3e13937-2c7c-4924-99ee-4089e2fdf67c",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Out of 5 people surveyed , [REDACTED] , [REDACTED] and [REDACTED] like apples . [REDACTED] and [REDACTED] like oranges . '"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "survey_text = ('Out of 5 people surveyed, James Robert,'\n",
        "               ' Julie Fuller and Benjamin Brooks like'\n",
        "               ' apples. Kelly Cox and Matthew Evans'\n",
        "               ' like oranges.')\n",
        "\n",
        "def replace_person_names(token):\n",
        "    if token.ent_iob != 0 and token.ent_type_ == 'PERSON':\n",
        "        return '[REDACTED] '\n",
        "    return str(token)+\" \"\n",
        "\n",
        "def redact_names(nlp_doc):\n",
        "    with nlp_doc.retokenize() as retokenizer:\n",
        "      for ent in nlp_doc.ents:\n",
        "        retokenizer.merge(ent)\n",
        "    tokens = map(replace_person_names, nlp_doc)\n",
        "    return \"\".join(tokens)\n",
        "\n",
        "survey_doc = nlp(survey_text)\n",
        "redact_names(survey_doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KriF66yM6Rlc",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "In this example, replace_person_names() uses ent_iob. It gives the IOB code of the named entity tag using inside-outside-beginning (IOB) tagging. Here, it can assume a value other than zero, because zero means that no entity tag is set.\n",
        "\n",
        "## 5. Pipelines\n",
        "\n",
        "spaCy actually runs all the tasks and tools in the pipeline automatically when you call the `nlp` functions.\n",
        "![Alt](https://d33wubrfki0l68.cloudfront.net/16b2ccafeefd6d547171afa23f9ac62f159e353d/48b91/pipeline-7a14d4edd18f3edfee8f34393bff2992.svg)\n",
        "\n",
        "In some situations, it may be useful to run only part of the standard pipeline and thus to disable some proesses. \n",
        "For example, we can disable POS tagging and parsing as follows: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8exOVjVV6Rlc",
        "outputId": "32670e32-cfe1-45ef-a0b2-0ea34c36877b",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('$9.4 million', 'MONEY'), ('the prior year', 'DATE'), ('$2.7 million', 'MONEY')]\n",
            "[('twelve billion dollars', 'MONEY'), ('1b', 'MONEY')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "texts = [\n",
        "    \"Net income was $9.4 million compared to the prior year of $2.7 million.\",\n",
        "    \"Revenue exceeded twelve billion dollars, with a loss of $1b.\",\n",
        "]\n",
        "\n",
        "alt_nlp = spacy.load(\"en_core_web_md\")\n",
        "for doc in nlp.pipe(texts, disable=[\"tagger\", \"parser\"]):\n",
        "    # Do something with the doc here\n",
        "    print([(ent.text, ent.label_) for ent in doc.ents])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHb6D16q6Rlc",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "You can find more details about pipelines and custom pipeline coponents in the spAcy documentation: https://spacy.io/usage/processing-pipelines\n",
        " \n",
        "\n",
        "## 6. Rule-based Matching\n",
        "**Rule-based matching** is one of the steps in extracting information from unstructured text. \n",
        "It’s used to identify and extract tokens and phrases according to patterns (such as lowercase)\n",
        " and grammatical features (such as part of speech).\n",
        "\n",
        "Rule-based matching can use regular expressions to extract entities (such as phone numbers)\n",
        " from an unstructured text. It’s different from extracting text using regular expressions only in the sense that regular expressions don’t consider the lexical and grammatical attributes of the text.\n",
        "\n",
        "With rule-based matching, you can extract a first name and a last name, which are always **proper nouns**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gUYs7E8A6Rlc",
        "outputId": "6f24e16a-feea-4832-ce96-c01f9e6826e4",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Gus Proto'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from spacy.matcher import Matcher\n",
        "matcher = Matcher(nlp.vocab)\n",
        "def extract_full_name(nlp_doc):\n",
        "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
        "    matcher.add('FULL_NAME', [pattern])\n",
        "    matches = matcher(nlp_doc)\n",
        "    for match_id, start, end in matches:\n",
        "        span = nlp_doc[start:end]\n",
        "        return span.text\n",
        "\n",
        "extract_full_name(about_doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0vyP7ul6Rld",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "In this example, pattern is a list of objects that defines the combination of tokens to be matched. Both POS tags in it are PROPN (proper noun). So, the pattern consists of two objects in which the POS tags for both tokens should be PROPN. This pattern is then added to Matcher using FULL_NAME and the the match_id. Finally, matches are obtained with their starting and end indexes.\n",
        "\n",
        "You can also use rule-based matching to extract phone numbers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_ZtIvGQF6Rld",
        "outputId": "065c9ec1-3dab-4d75-accc-670707495ede",
        "pycharm": {
          "name": "#%% \n"
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'(123) 456-789'"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from spacy.matcher import Matcher\n",
        "matcher = Matcher(nlp.vocab)\n",
        "conference_org_text = ('There is a developer conference'\n",
        "    'happening on 21 July 2019 in London. It is titled'\n",
        "    ' \"Applications of Natural Language Processing\".'\n",
        "    ' There is a helpline number available'\n",
        "    ' at (123) 456-789')\n",
        "\n",
        "def extract_phone_number(nlp_doc):\n",
        "    pattern = [{'ORTH': '('}, {'SHAPE': 'ddd'},\n",
        "               {'ORTH': ')'}, {'SHAPE': 'ddd'},\n",
        "               {'ORTH': '-', 'OP': '?'},\n",
        "               {'SHAPE': 'ddd'}]\n",
        "    matcher.add('PHONE_NUMBER', [pattern])\n",
        "    matches = matcher(nlp_doc)\n",
        "    for match_id, start, end in matches:\n",
        "        span = nlp_doc[start:end]\n",
        "        return span.text\n",
        "\n",
        "conference_org_doc = nlp(conference_org_text)\n",
        "extract_phone_number(conference_org_doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKhkt_lY6Rld"
      },
      "source": [
        "In this example, only the pattern is updated in order to match phone numbers from the previous example. Here, some attributes of the token are also used:\n",
        "\n",
        " - `ORTH` gives the exact text of the token.\n",
        " - `SHAPE` transforms the token string to show orthographic features.\n",
        " - `OP` defines operators. Using ? as a value means that the pattern is optional, meaning it can match 0 or 1 times.\n",
        " \n",
        " Rule-based matching helps you identify and extract tokens and phrases according to lexical patterns (such as lowercase) and grammatical features(such as part of speech).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6ZQp-eaXcHa"
      },
      "source": [
        "##### Verb Phrase Detecton\n",
        "\n",
        "A verb phrase is a syntactic unit composed of at least one verb. This verb can be followed by other chunks, such as noun phrases. Verb phrases are useful for understanding the actions that nouns are involved in.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEHez7UqXilt",
        "outputId": "bd7a2933-ac4f-46e3-9b4d-805f01f8b9d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sat, quickly ran, jumped, is writing]\n"
          ]
        }
      ],
      "source": [
        "import spacy   \n",
        "from spacy.matcher import Matcher\n",
        "from spacy.util import filter_spans\n",
        "\n",
        "sentence = 'The cat sat on the mat. He quickly ran to the market. The dog jumped into the water. The author is writing a book.'\n",
        "pattern = [{'POS': 'VERB', 'OP': '?'},\n",
        "           {'POS': 'ADV', 'OP': '*'},\n",
        "           {'POS': 'AUX', 'OP': '*'},\n",
        "           {'POS': 'VERB', 'OP': '+'}]\n",
        "\n",
        "# instantiate a Matcher instance\n",
        "matcher = Matcher(nlp.vocab)\n",
        "matcher.add(\"Verb phrase\", [pattern])\n",
        "\n",
        "doc = nlp(sentence) \n",
        "# call the matcher to find matches \n",
        "matches = matcher(doc)\n",
        "spans = [doc[start:end] for _, start, end in matches]\n",
        "\n",
        "print (filter_spans(spans)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCUIgRx-XxCO"
      },
      "source": [
        "In this example, the verb phrase indicates that something will be introduced.\n",
        "The above code extracts all the verb phrases using a pattern of POS tags. \n",
        "You can tweak the pattern for verb phrases depending upon your use case."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
