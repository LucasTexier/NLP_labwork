{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modelling: ngram models, distributional semantics and word embeddings\n",
    "\n",
    "## 1. Traditional Language Modelling (n-gram models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U pip\n",
    "!pip install -U dill\n",
    "!pip install -U nltk==3.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowadays, everything seems to be going neural... \n",
    "\n",
    "Traditionally, we can use n-grams to generate language models to predict which word comes next given a history of words. \n",
    "\n",
    "We'll use the `lm` module in `nltk` to get a sense of how non-neural language modelling is done.\n",
    "\n",
    "(**Source:** The content in this notebook is largely based on [language model tutorial in NLTK documentation by Ilia Kurenkov](https://github.com/nltk/nltk/blob/develop/nltk/lm/__init__.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to train a bigram model, we need to turn this text into bigrams. Here's what the first sentence of our text would look like if we use the `ngrams` function from NLTK for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text = [['a', 'b', 'c'], ['a', 'c', 'd', 'c', 'e', 'f']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list(bigrams(text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list(ngrams(text[1], n=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how \"b\" occurs both as the first and second member of different bigrams but \"a\" and \"c\" don't? \n",
    "\n",
    "Wouldn't it be nice to somehow indicate how often sentences start with \"a\" and end with \"c\"?\n",
    "\n",
    "\n",
    "A standard way to deal with this is to add special \"padding\" symbols to the sentence before splitting it into ngrams. Fortunately, NLTK also has a function for that, let's see what it does to the first sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.util import pad_sequence\n",
    "list(pad_sequence(text[0],\n",
    "                  pad_left=True, left_pad_symbol=\"<s>\",\n",
    "                  pad_right=True, right_pad_symbol=\"</s>\",\n",
    "                  n=2)) # The n order of n-grams, if it's 2-grams, you pad once, 3-grams pad twice, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "padded_sent = list(pad_sequence(text[0], pad_left=True, left_pad_symbol=\"<s>\", \n",
    "                                pad_right=True, right_pad_symbol=\"</s>\", n=2))\n",
    "list(ngrams(padded_sent, n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list(pad_sequence(text[0],\n",
    "                  pad_left=True, left_pad_symbol=\"<s>\",\n",
    "                  pad_right=True, right_pad_symbol=\"</s>\",\n",
    "                  n=3)) # The n order of n-grams, if it's 2-grams, you pad once, 3-grams pad twice, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "padded_sent = list(pad_sequence(text[0], pad_left=True, left_pad_symbol=\"<s>\", \n",
    "                                pad_right=True, right_pad_symbol=\"</s>\", n=3))\n",
    "list(ngrams(padded_sent, n=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `n` argument, that tells the function we need padding for bigrams.\n",
    "\n",
    "Now, passing all these parameters every time is tedious and in most cases they can be safely assumed as defaults anyway.\n",
    "\n",
    "Thus the `nltk.lm` module provides a convenience function that has all these arguments already set while the other arguments remain the same as for `pad_sequence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "list(pad_both_ends(text[0], n=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the two parts discussed so far we get the following preparation steps for one sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list(bigrams(pad_both_ends(text[0], n=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our model more robust we could also train it on unigrams (single words) as well as bigrams, its main source of information.\n",
    "NLTK once again helpfully provides a function called `everygrams`.\n",
    "\n",
    "While not the most efficient, it is conceptually simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.util import everygrams\n",
    "padded_bigrams = list(pad_both_ends(text[0], n=2))\n",
    "list(everygrams(padded_bigrams, max_len=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost ready to start counting ngrams, just one more step left.\n",
    "\n",
    "During training and evaluation our model will rely on a vocabulary that defines which words are \"known\" to the model.\n",
    "\n",
    "To create this vocabulary we need to pad our sentences (just like for counting ngrams) and then combine the sentences into one flat stream of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import flatten\n",
    "list(flatten(pad_both_ends(sent, n=2) for sent in text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most cases we want to use the same text as the source for both vocabulary and ngram counts.\n",
    "\n",
    "Now that we understand what this means for our preprocessing, we can simply import a function that does everything for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "train, vocab = padded_everygram_pipeline(2, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as to avoid re-creating the text in memory, both `train` and `vocab` are lazy iterators. They are evaluated on demand at training time.\n",
    "\n",
    "For the sake of understanding the output of `padded_everygram_pipeline`, we'll \"materialize\" the lazy iterators by casting them into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_ngrams, padded_sentences = padded_everygram_pipeline(2, text)\n",
    "for ngramlize_sent in training_ngrams:\n",
    "    print(list(ngramlize_sent))\n",
    "    print()\n",
    "print('#############')\n",
    "list(padded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets get some real data and tokenize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#NB: You need to run nltk.download(), a window will appear, go in the models tab and install the punkt_tokenizer model. Then close the window. \n",
    "#If you don't do this, it will fallback on a very trivial regex based tokenization\n",
    "\n",
    "try: # Use the default NLTK tokenizer.\n",
    "    from nltk import word_tokenize, sent_tokenize \n",
    "    # Testing whether it works. \n",
    "    # Sometimes it doesn't work on some machines because of setup issues.\n",
    "    word_tokenize(sent_tokenize(\"This is a foobar sentence. Yes it is.\")[0])\n",
    "except: # Use a naive sentence tokenizer and toktok.\n",
    "    import re\n",
    "    from nltk.tokenize import ToktokTokenizer\n",
    "    # See https://stackoverflow.com/a/25736515/610569\n",
    "    sent_tokenize = lambda x: re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', x)\n",
    "    # Use the toktok tokenizer that requires no dependencies.\n",
    "    toktok = ToktokTokenizer()\n",
    "    word_tokenize = word_tokenize = toktok.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import io #codecs\n",
    "\n",
    "\n",
    "# Text version of https://kilgarriff.co.uk/Publications/2005-K-lineer.pdf\n",
    "if os.path.isfile('language-never-random.txt'):\n",
    "    with io.open('language-never-random.txt', encoding='utf8') as fin:\n",
    "        text = fin.read()\n",
    "else:\n",
    "    url = \"https://gist.githubusercontent.com/alvations/53b01e4076573fea47c6057120bb017a/raw/b01ff96a5f76848450e648f35da6497ca9454e4a/language-never-random.txt\"\n",
    "    text = requests.get(url).content.decode('utf8')\n",
    "    with io.open('language-never-random.txt', 'w', encoding='utf8') as fout:\n",
    "        fout.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize the text.\n",
    "tokenized_text = [list(map(str.lower, word_tokenize(sent))) \n",
    "                  for sent in sent_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess the tokenized text for 3-grams language modelling\n",
    "n = 3\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an N-gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having prepared our data we are ready to start training a model. As a simple example, let us train a Maximum Likelihood Estimator (MLE).\n",
    "\n",
    "We only need to specify the highest ngram order to instantiate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "model = MLE(n) # Lets train a 3-grams model, previously we set n=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the MLE model, creates an empty vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... which gets filled as we fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(train_data, padded_sents)\n",
    "print(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary helps us handle words that have not occurred during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(model.vocab.lookup(tokenized_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# If we lookup the vocab on unseen sentences not from the training data, \n",
    "# it automatically replace words not in the vocabulary with `<UNK>`.\n",
    "print(model.vocab.lookup('language is never random lah .'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, in some cases we want to ignore words that we did see during training but that didn't occur frequently enough, to provide us useful information. \n",
    "\n",
    "You can tell the vocabulary to ignore such words using the `unk_cutoff` argument for the vocabulary lookup, To find out how that works, check out the docs for the [`nltk.lm.vocabulary.Vocabulary` class](https://github.com/nltk/nltk/blob/develop/nltk/lm/vocabulary.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** For more sophisticated ngram models, take a look at [these objects from `nltk.lm.models`](https://github.com/nltk/nltk/blob/develop/nltk/lm/models.py):\n",
    "\n",
    " - `Lidstone`: Provides Lidstone-smoothed scores.\n",
    " - `Laplace`: Implements Laplace (add one) smoothing.\n",
    " - `InterpolatedLanguageModel`: Logic common to all interpolated language models (Chen & Goodman 1995).\n",
    " - `WittenBellInterpolated`: Interpolated version of Witten-Bell smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the N-gram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to ngram models the training boils down to counting up the ngrams from the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(model.counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides a convenient interface to access counts for unigrams..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.counts['language'] # i.e. Count('language')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and bigrams for the phrase \"language is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.counts[['language']]['is'] # i.e. Count('is'|'language')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and trigrams for the phrase \"language is never\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.counts[['language', 'is']]['never'] # i.e. Count('never'|'language is')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so on. However, the real purpose of training a language model is to have it score how probable words are in certain contexts.\n",
    "\n",
    "This being MLE, the model returns the item's relative frequency as its score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.score('language') # P('language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.score('is', 'language'.split())  # P('is'|'language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.score('never', 'language is'.split())  # P('never'|'language is')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Items that are not seen during training are mapped to the vocabulary's \"unknown label\" token.  This is \"<UNK>\" by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.score(\"<UNK>\") == model.score(\"lah\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.score(\"<UNK>\") == model.score(\"leh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.score(\"<UNK>\") == model.score(\"lor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid underflow when working with many small score values it makes sense to take their logarithm. \n",
    "\n",
    "For convenience this can be done with the `logscore` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.logscore(\"never\", \"language is\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation using N-gram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One cool feature of ngram models is that they can be used to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(model.generate(20, random_seed=7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do some cleaning to the generated tokens to make it human-like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_sent(model, num_words, random_seed=42):\n",
    "    \"\"\"\n",
    "    :param model: An ngram language model from `nltk.lm.model`.\n",
    "    :param num_words: Max no. of words to generate.\n",
    "    :param random_seed: Seed value for random.\n",
    "    \"\"\"\n",
    "    content = []\n",
    "    for token in model.generate(num_words, random_seed=random_seed):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generate_sent(model, 20, random_seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(model.generate(28, random_seed=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generate_sent(model, 28, random_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generate_sent(model, 20, random_seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generate_sent(model, 20, random_seed=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generate_sent(model, 20, random_seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model \n",
    "\n",
    "The native Python's pickle may not save the lambda functions in the  model, so we can use the `dill` library in place of pickle to save and load the language model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import dill as pickle \n",
    "\n",
    "with open('kilgariff_ngram_model.pkl', 'wb') as fout:\n",
    "    pickle.dump(model, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('kilgariff_ngram_model.pkl', 'rb') as fin:\n",
    "    model_loaded = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generate_sent(model_loaded, 20, random_seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets try some generating with Donald Trump data!!!\n",
    "\n",
    "\n",
    "**Dataset:** https://www.kaggle.com/kingburrito666/better-donald-trump-tweets#Donald-Tweets!.csv\n",
    "\n",
    "\n",
    "In this part, I'll be munging that data as how I would be doing it at work. \n",
    "I've really no seen the data before but I hope this session would be helpful for you to see how to approach new datasets with the skills you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('trump_tweets.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trump_corpus = list(df['Tweet_Text'].apply(word_tokenize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess the tokenized text for 3-grams language modelling\n",
    "n = 3\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, trump_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "trump_model = MLE(n) # Lets train a 3-grams model, previously we set n=3\n",
    "trump_model.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generate_sent(trump_model, num_words=20, random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generate_sent(trump_model, num_words=10, random_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generate_sent(trump_model, num_words=50, random_seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(generate_sent(trump_model, num_words=100, random_seed=52))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Latent Word Models (aka. Distributional Semantics, aka. Word Embeddings)\n",
    "\n",
    "The idea of word embeddings was born in the 1990 with the advant of so-called distributional\n",
    "semantics methods that are inspired by the distributional hypothesis by (Firth, 1957).\n",
    "\n",
    "\"A word is known by the company it keeps\" \n",
    "\n",
    "### Latent Semantic Analysis \n",
    "In IR, we are interested in building document models, that is the distributions of occurence of words in\n",
    "documents. In that context LSA consists in building a term-document matrix, applying tf-idf or okapi-bm25 and then \n",
    "reducing the dimensionality with SVD or PCA to obtain a latent document model. \n",
    "\n",
    "As far surface-level semantics is concerned (lexical semantics), we want to capture the contexts \n",
    "in which words appear (we count co-occurences between words in a particular context), to do so\n",
    "instead of computing word-documents matrices, we compute word-word matrices representing the cooccurences.\n",
    "\n",
    "These coocurences are computed within a sliding window over the text (can be a few words around the target \n",
    "word or a sentence) on the basis of cooccurence frequency or on the frequency of dependences between the words. \n",
    "\n",
    "Once we have computed a word-word (or term-term) matrix, we can normalize it using tf-idf and use a dimentionality \n",
    "reduction technique to obtain compact latent representations. \n",
    "\n",
    "We can compute a sparse word-word co-occurence matrix weighted with tf-idf as follows in python using NLTK: here the documents are the sentences, the twist is that the tf calculation is based on cooccurences within the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import scipy\n",
    "import math\n",
    "def create_cooccurrence_matrix(sentences, window_size=5, use_tfidf=True):\n",
    "    vocabulary = {}\n",
    "    token_idfs = []\n",
    "    data = []\n",
    "    row = []\n",
    "    col = []\n",
    "\n",
    "    tokenizer = nltk.tokenize.word_tokenize  # We could also use SpaCy here\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        tokens = [token for token in tokenizer(sentence) if token != u\"\"]\n",
    "\n",
    "        for pos, token in enumerate(tokens):\n",
    "            i = vocabulary.setdefault(token, len(vocabulary))\n",
    "            start = max(0, pos - window_size)  # window start: current position - window size\n",
    "            end = min(len(tokens), pos + window_size + 1)  # window end: current position + window size\n",
    "            for pos2 in range(start, end):  # Sliding over the window and counting\n",
    "                if pos2 == pos:\n",
    "                    continue\n",
    "                j = vocabulary.setdefault(tokens[pos2], len(vocabulary))\n",
    "                data.append(1.)\n",
    "                row.append(i)\n",
    "                col.append(j)\n",
    "\n",
    "\n",
    "    cooccurrence_matrix_sparse = scipy.sparse.coo_matrix((data, (row, col)))  # Transforming list into sparse matrix\n",
    "\n",
    "    if use_tfidf:\n",
    "        N = len(sentences)\n",
    "        tf_idf_matrix = scipy.sparse.csr_matrix((data, (row, col)))\n",
    "        total_counts = cooccurrence_matrix_sparse.sum(axis=0).tolist()[0]\n",
    "        for token in vocabulary.keys():\n",
    "            token_df = len([sentence for sentence in sentences if token in sentence])\n",
    "            token_idfs.append(math.log(N / token_df))\n",
    "\n",
    "        # Computing tf-idf on coocurence counts (here tf = coocurrence count)\n",
    "        for i, j, v in zip(cooccurrence_matrix_sparse.row, cooccurrence_matrix_sparse.col, cooccurrence_matrix_sparse.data):\n",
    "            tf_idf_matrix[i, j] = (0.5 + (0.5 * v / total_counts[i])) * token_idfs[j]\n",
    "        return vocabulary, tf_idf_matrix\n",
    "    else:\n",
    "        return vocabulary, cooccurrence_matrix_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "sentences = ['I love nlp',    'I love to learn',\n",
    "             'nlp is future', 'nlp is cool']\n",
    "\n",
    "#We generate the matrix\n",
    "vocabs,co_occ = create_cooccurrence_matrix(sentences, use_tfidf=True)\n",
    "\n",
    "df_co_occ  = pandas.DataFrame(co_occ.todense(),\n",
    "                          index=vocabs.keys(),\n",
    "                          columns = vocabs.keys())\n",
    "\n",
    "#We sort the dimensions to be in the same alphabetical order as the vocabulary \n",
    "df_co_occ = df_co_occ.sort_index()[sorted(vocabs.keys())]\n",
    "\n",
    "#Visualizing\n",
    "df_co_occ.style.applymap(lambda x: 'color: red' if x>0 else '')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we compute vectors over the entire vocabulary space, if the vocabulary is very large (e.g. millions of words), the vectors would be to large and sparse to be practical. \n",
    "A classical technique in information retrieval, but also in what we call distributional semantics (based on co-occurence information), is to apply dimentionality reduction and to keep only the most important components. \n",
    "\n",
    "Singular Value Decomposition is the standard choice, it's an approach equivalent to PCA (after normalization) and in fact, virtually all implementations of PCA use the SVD decomposition!\n",
    "SVD is a matrix decomposition technique formulated as (on reals): \n",
    "\n",
    "$D=U\\Sigma V^T$\n",
    "Where $D$ is the original data, $\\Sigma$ is a diagonal matrix, and \\(U\\) and \\(V\\) are two orthogonal matrices. \n",
    "\n",
    "We estimate SVD on the co_occurance matrix with $k$ components and the $U$ matrix contains the projected vector space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Computing Sparse SVD with 5 components (embedding vectors of dimension 5):\n",
    "from scipy.sparse.linalg import svds\n",
    "u, s, vt = svds(co_occ, k=5)\n",
    "\n",
    "#U contains the embeddings\n",
    "df_embeddings = pandas.DataFrame(u,\n",
    "                          index=vocabs.keys())\n",
    "df_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `u` matrix will contains our embedding vectors, each row corresponds to the embedding vector of a word of \n",
    "the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "future_vec = u[vocabs[\"future\"]]\n",
    "future_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can thus compute distances and similarities between vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "print(cosine(u[vocabs[\"love\"]], u[vocabs[\"I\"]])) #Smaller distance = words are closer\n",
    "\n",
    "print(cosine(u[vocabs[\"love\"]], u[vocabs[\"nlp\"]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Word Embeddings\n",
    "With the creation of Word2Vec, Neural Word Embeddings revolutionized word embeddings by framing the learning of the embeddings as a predction task: \n",
    "- Predict a word from the context (Continuous Bag of Words)\n",
    "- Predict the context from a target word (Skip-Gram Model)\n",
    "\n",
    "Computing PCA/SVD on a huge matrix doesn't scale and is not only computationally costly but also difficult to distribute and run in parallel. \n",
    "\n",
    "Word2Vec and its simple neural architecture with NegSampling, allowed an exponential speed-up of the computation of word embeddings models, which in turn allowed to train models on Billion-Word corpora. See: Reading material 2 on word embeddings. \n",
    "For more context and history, please read: http://jalammar.github.io/illustrated-word2vec/\n",
    "\n",
    "\n",
    "You have already seen how to load a pre-trained word embeddings model with TensorFlow2 and Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "#Loading a hub model (pretrained word vector)\n",
    "hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/Wiki-words-250/2\", \n",
    "                           input_shape=[], \n",
    "                           dtype=tf.string, \n",
    "                           trainable=False) #If we just use the vectors on their own, no need to make them trainable\n",
    "\n",
    "# Cosine similarity between words\n",
    "tf.print(\"dog & cat: \", -losses.CosineSimilarity()(hub_layer([\"cat\"]),hub_layer([\"dog\"])))\n",
    "tf.print(\"mom & dad: \", -losses.CosineSimilarity()(hub_layer([\"mom\"]),hub_layer([\"dad\"])))\n",
    "tf.print(\"house & tree: \", -losses.CosineSimilarity()(hub_layer([\"house\"]),hub_layer([\"tree\"])))\n",
    "print(hub_layer([\"cat\"]).numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers & Sentence Embeddings \n",
    "\n",
    "We will see Transformer Language Models in detail in the next tutorial session and apply them to text generation, however it is important to introduce their use to obtain word and sentence embeddings and exploit those for various applications. \n",
    "\n",
    "#### Embedding text with a pretrained transformer (hugging face transformers)\n",
    "Transformers have revolutionized many NLP tasks, by providing a way of building model that can capture many aspects of language (except true semantics and pragmatics), through multi-task modular pre-training capabilities. \n",
    "Most large-scale pre-trained deep \"language\" models are for the most part encoder-decoder architectures stacking transformers that take in tokenized text encoded in a way that captures word position (positonal embeddings) and output, depeding on the target pre-training task, contextualized vectors for each token, a pooled vector specific for each classification problems, other tasks specific values (start/end offsets for a named entity recognition task). \n",
    "Beyond using transformers in the usual calssification tasks, one can also extract contextualised word vectors or even pooled sequence vectors that typically capture more information than classical word embeddings or distributional semantics models. \n",
    "\n",
    "Let's see how to do that with the transformers library. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you don't have pytorch, install this. \n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we need to install the transformer library\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use autoclasses from transformers that instantiate the right neural network modules based on the specified pre-trained model. \n",
    "\n",
    "We will select one of the official pre-trained models https://huggingface.co/transformers/v3.3.1/pretrained_models.html, but there are many more available in the model hub https://huggingface.co/models.\n",
    "\n",
    "Distillbert is a compressed BERT model that is much smaller but with only limited performance degradation. We can use a multilingual version to make it applicable on several common languages like French and English. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can first tokenize the text by using the tokenizer. This is a class instance that overrides the `__call__` method, allowing us to call the object like a function.\n",
    "`return_tensors='pt'` tells the tokenizer to return a pytorch tensor, which is required if we are going tu use a pytorch model. Transformers also support Tensorflow models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input = tokenizer(\"Transformers have revolutionized many NLP tasks, by providing a way of building model that can capture many aspects of language\", padding=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the actual tokens by using the `convert_ids_to_tokens` method of the tokenizer. Just make sure to convert the input ids to a list\n",
    "Also notice how some words are sgmented into pieces. For example, `revolution` `##ized` or `NL` `##P`. `##` indicated that a word token has been split into pieces. This tokenization technique is called piece-wise tokenization and is used by many transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer.convert_ids_to_tokens(input['input_ids'].tolist()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the model to embed the text encoded as token ids in the input variable! We have to use `**input` to flatten the dictionary so that each key/value pair is passed as an argument to the call. Without any other options, we only get `last_hidden_state` as an output of the model, which gives us a vector for each of the tokens. For classification tasks, the output of the first token (always `[CLS]`) is a class-specific feature vector, but this isn't always the best way of getting additional information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**input)\n",
    "print(output.last_hidden_state.shape)\n",
    "cls_vector = output.last_hidden_state.squeeze()[0]\n",
    "cls_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better way of having one vector is aggregating individual word vectors. This can be done manually by appying some aggregation function. Endless possibilities, but the arithmetic mean, the sum or the max are very typical examples. The naïve version below will work, but proper pooling should take into account attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_matrix = output.last_hidden_state.squeeze()[1:] # Without the CLS vector\n",
    "print(output_matrix.shape)\n",
    "sum_pooled = output_matrix.sum(axis=0)\n",
    "mean_pooled = output_matrix.mean(axis=0)\n",
    "max_pooled = output_matrix.max(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we use those attention weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "print(output)\n",
    "mean_pooled = mean_pooling(output, input.attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Sentence embeddings for text similarity and search \n",
    "\n",
    "SentenceTransformers are aimed at producing efficient sentence embeddings in order to compute similarity scores between setences (textual similarity). To do so SentenceTransformers uses BERT to encode two sentences, but then uses a cosine similarity loss function to train an encoder on top of BERT to rank sentences by similarity. They use either a Siamois inspired architecture called the BiEncoder or directly fine-tune BERT to bring sentences closer in its internal representation.\n",
    "\n",
    "![Bi-encoder v.s. Cross-encoder](https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/Bi_vs_Cross-Encoder.png)\n",
    "\n",
    "Contrarily to BERT that produces one vector per word and for the [CLS] token, after training, SentenceBERT can produce a single sentence embedding that is more adapted for text similarity tasks. The model uses trainable pooling layers to go from word vectors to a single aggregate vector. \n",
    "\n",
    "Let's install it first, load it and then embed our first sentence! Sentence-transformers can be used directly through the dedicated library or through the transformers library. We will give both examples. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"Jacques à joyeusement passé la tondeuse à gazon ce matin sous un soleil chatoyant.\", \"Jack merrily mowed the lawn this morning under a shimmering sun.\"]\n",
    "embeddings = model.encode(sentences, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "cos_sim = lambda a,b: dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "cos_sim(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also directly use sentence transformers through the unified interface the transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('AIDA-UPM/mstsb-paraphrase-multilingual-mpnet-base-v2')\n",
    "model = AutoModel.from_pretrained('AIDA-UPM/mstsb-paraphrase-multilingual-mpnet-base-v2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "model_output = model(**encoded_input)\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "#print(sentence_embeddings[1])\n",
    "from torch import nn\n",
    "t_cos = nn.CosineSimilarity(dim=0)\n",
    "print(t_cos(sentence_embeddings[0],sentence_embeddings[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Let's extend our text classification pipeline!\n",
    "\n",
    "Now that you have seen how to make operational use of language modelling approaches, \n",
    "create three variants where you use: \n",
    "\n",
    "1. an n-gram model \n",
    "2. Word Embeddings (pre-trained) \n",
    "3. Sentence embeddings\n",
    "\n",
    "You can use those as features to a standard skilearn classfier for example. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
